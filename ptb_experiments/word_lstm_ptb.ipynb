{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network for word-level language modelling\n",
    "\n",
    "Replicating the results from:\n",
    "\n",
    "Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. \"Recurrent neural network regularization.\" arXiv preprint arXiv:1409.2329 (2014).\n",
    "\n",
    "on the Penn Tree Bank dataset (downloaded from http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz)\n",
    "\n",
    "using theano and lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 640 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.ifelse import ifelse\n",
    "import lasagne\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.utils import unroll_scan\n",
    "\n",
    "from lasagne.layers.input import InputLayer\n",
    "\n",
    "from lasagne.layers.recurrent import Gate\n",
    "\n",
    "# This is an LSTM layer that inherits from the lasagne implementation\n",
    "# it allows to access the cell state\n",
    "from recurrent_extend import LSTMLayerWithState\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(11071988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_lstm_ptb: model size small, learning rate 1.000, optimization sgd, b constant_f0\n"
     ]
    }
   ],
   "source": [
    "datapath = './data/'\n",
    "model_size = 'small' # small, medium or large\n",
    "learning_rate = 1\n",
    "optim_type = 'sgd' # sgd or adadelta\n",
    "b_init = 'constant_f0' # constant, constant_f0 or uniform\n",
    "\n",
    "print('word_lstm_ptb: model size %s, learning rate %.3f, optimization %s, b %s' %(model_size,learning_rate,optim_type,b_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(datapath+'/ptb.train.txt', 'r') as f:\n",
    "    tokens = f.read().replace('\\n','<eos>').split(' ')[1:]\n",
    "with open(datapath+'/ptb.valid.txt', 'r') as f:\n",
    "    tokens_valid = f.read().replace('\\n','<eos>').split(' ')[1:]\n",
    "with open(datapath+'/ptb.test.txt', 'r') as f:\n",
    "    tokens_test = f.read().replace('\\n','<eos>').split(' ')[1:]    \n",
    "\n",
    "def build_vocabulary(tokens):\n",
    "    counter = collections.Counter(tokens)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "vocab = build_vocabulary(tokens)\n",
    "data = [vocab[word] for word in tokens if word in vocab]\n",
    "data_valid = [vocab[word] for word in tokens_valid if word in vocab]\n",
    "data_test = [vocab[word] for word in tokens_test if word in vocab]\n",
    "sortindex = np.argsort(list(vocab.values()))\n",
    "words = np.array(list(vocab.keys()))[sortindex]\n",
    "vocab_size = len(list(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_producer(data, batch_size, n_batch=None):\n",
    "    datanp = np.array(data).astype('int32')\n",
    "    batch_len = int(np.floor(datanp.shape[0]/batch_size))\n",
    "    datanp = np.reshape(datanp[0 : batch_size * batch_len],\n",
    "                        [batch_size, batch_len])\n",
    "    epoch_size = (batch_len - 1) // seq_length\n",
    "    pointers = np.arange(0,batch_len-seq_length,seq_length)\n",
    "    if n_batch is not None:\n",
    "        pointers = pointers[:n_batch]\n",
    "    for (ii, pointer) in enumerate(pointers):\n",
    "        x = datanp[:,pointer:pointer+seq_length]\n",
    "        y = datanp[:,pointer+1:pointer+seq_length+1]\n",
    "        yield x,y,(ii/len(pointers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "n_layer = 2\n",
    "grad_clip = 5\n",
    "# grad_clip = grad_clip*batch_size\n",
    "\n",
    "if model_size=='large':\n",
    "    # Large\n",
    "    embed_size = 1500\n",
    "    seq_length = 35\n",
    "    init_range = .04\n",
    "    dropout_rate = .65\n",
    "    num_epoch = 55\n",
    "    max_epoch = 14\n",
    "    decay = 1.15\n",
    "elif model_size=='medium':\n",
    "    # Medium\n",
    "    embed_size = 650\n",
    "    seq_length = 35\n",
    "    init_range = .05\n",
    "    dropout_rate = .5\n",
    "    num_epoch = 39\n",
    "    max_epoch = 6\n",
    "    decay = 1.25\n",
    "elif model_size=='small':\n",
    "    # Small\n",
    "    embed_size = 200\n",
    "    seq_length = 20\n",
    "    init_range = .1\n",
    "    dropout_rate = 0.\n",
    "    num_epoch = 13\n",
    "    max_epoch = 4\n",
    "    decay = 2\n",
    "elif model_size=='test':\n",
    "    embed_size = 30\n",
    "    seq_length = 35\n",
    "    init_range = .1\n",
    "    dropout_rate = 0.\n",
    "    num_epoch = 1\n",
    "    batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (test_x,test_y,p) in data_producer(data_test, batch_size, 3):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_fcn(val_shape, init_range, init_range_upper=None):\n",
    "    if init_range_upper is None:\n",
    "        val = np.random.uniform(-init_range,init_range,\n",
    "                                val_shape).astype(theano.config.floatX)\n",
    "    else:\n",
    "        val = np.random.uniform(init_range,init_range_upper,\n",
    "                                val_shape).astype(theano.config.floatX)\n",
    "    return val\n",
    "\n",
    "def zero_fcn(val_shape):\n",
    "    val = np.zeros(val_shape).astype(theano.config.floatX)\n",
    "    return val\n",
    "\n",
    "def ones_fcn(val_shape):\n",
    "    val = np.ones(val_shape).astype(theano.config.floatX)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word embedding\n",
    "embed = theano.shared(name='embedding variable', value=init_fcn((vocab_size,embed_size),init_range))\n",
    "# cell and hidden state initial values\n",
    "c_init = []; h_init = []\n",
    "for iLayer in range(n_layer):\n",
    "    c_init.append(theano.shared(zero_fcn((batch_size,embed_size))))\n",
    "    h_init.append(theano.shared(zero_fcn((batch_size,embed_size))))\n",
    "# parameters for ingate and outgate:\n",
    "def io_gate_parameters():\n",
    "    g = Gate(W_in=init_fcn((embed_size,embed_size),init_range),\n",
    "             W_hid=init_fcn((embed_size,embed_size),init_range),\n",
    "             W_cell=None, b=zero_fcn((embed_size)))        \n",
    "    return g\n",
    "# parameters for forgetgate:\n",
    "if b_init=='constant_f0':\n",
    "    def f_gate_parameters():\n",
    "        g = Gate(W_in=init_fcn((embed_size,embed_size),init_range),\n",
    "                 W_hid=init_fcn((embed_size,embed_size),init_range),\n",
    "                 W_cell=None, b=zero_fcn((embed_size)))         \n",
    "        return g\n",
    "else:\n",
    "    def f_gate_parameters():\n",
    "        g = Gate(W_in=init_fcn((embed_size,embed_size),init_range),\n",
    "                 W_hid=init_fcn((embed_size,embed_size),init_range),\n",
    "                 W_cell=None, b=ones_fcn((embed_size)))         \n",
    "        return g    \n",
    "# parameters for cell:\n",
    "def cell_parameters():\n",
    "    c = Gate(W_in=init_fcn((embed_size,embed_size),init_range),\n",
    "             W_hid=init_fcn((embed_size,embed_size),init_range),\n",
    "             W_cell=None, b=zero_fcn((embed_size)),\n",
    "             nonlinearity=nonlinearities.tanh)\n",
    "    return c\n",
    "gates = []\n",
    "for iLayer in range(n_layer):\n",
    "    gates.append([])\n",
    "    gates[iLayer].append(io_gate_parameters())\n",
    "    gates[iLayer].append(cell_parameters())\n",
    "    gates[iLayer].append(f_gate_parameters())\n",
    "    gates[iLayer].append(io_gate_parameters())\n",
    "# output embeddings\n",
    "W_hy = theano.shared(init_fcn((embed_size,vocab_size),init_range),name='output w')\n",
    "b_y = theano.shared(zero_fcn(vocab_size),name='output b')\n",
    "\n",
    "if b_init=='uniform':\n",
    "    for iLayer in range(n_layer):\n",
    "        for iGate in range(4):\n",
    "            gates[iLayer][iGate].b = init_fcn((embed_size),init_range)\n",
    "    b_y.set_value(init_fcn(vocab_size,init_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.imatrix('input variable').astype('int32')\n",
    "drate = theano.shared(dropout_rate).astype(T.config.floatX)\n",
    "input_embed = embed[input_var].reshape((input_var.shape[0], seq_length, embed_size))\n",
    "l_in = InputLayer(shape=(batch_size, seq_length, embed_size),\n",
    "                                 input_var=input_embed, name='input layer')\n",
    "l_in_drop = lasagne.layers.dropout(l_in, p=drate, name='input dropout layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f_input_embed = theano.function([input_var], input_embed, allow_input_downcast=True)\n",
    "# x_eval = f_input_embed(test_x)\n",
    "# len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_lstm = []; l_lstm_drop = [];\n",
    "l_c_init = []; l_h_init = []\n",
    "l_lstm_in = [l_in_drop]\n",
    "\n",
    "for iLayer in range(n_layer):\n",
    "    l_c_init.append(InputLayer(shape=(batch_size, embed_size), input_var=c_init[iLayer]))\n",
    "    l_h_init.append(InputLayer(shape=(batch_size, embed_size), input_var=h_init[iLayer])) \n",
    "    l_lstm.append(LSTMLayerWithState(l_lstm_in[-1], embed_size, ingate = gates[iLayer][0], \n",
    "                                     cell = gates[iLayer][1], forgetgate = gates[iLayer][2],\n",
    "                                     outgate = gates[iLayer][3], peepholes=False,\n",
    "                                     unroll_scan=True, name='lstm%d layer' %(iLayer+1),\n",
    "                                     nonlinearity=nonlinearities.tanh,\n",
    "                                     cell_init=l_c_init[-1], hid_init=l_h_init[-1]))\n",
    "    l_lstm_drop.append(lasagne.layers.dropout(l_lstm[-1], p=drate, \n",
    "                                              name='lstm%d dropout layer'  %(iLayer+1)))       \n",
    "    if iLayer<n_layer-1:\n",
    "        l_lstm_in.append(l_lstm_drop[-1])\n",
    "lstm_out = lasagne.layers.get_output(l_lstm_drop[-1])\n",
    "# lstm_out_det = lasagne.layers.get_output(l_lstm[-1],deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_lstm = lasagne.layers.get_all_params(l_lstm[-1])\n",
    "# p_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get LSTM cell and hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cellsc = dict()\n",
    "hid_out = []; cell_out = []\n",
    "\n",
    "for iLayer in range(n_layer):\n",
    "    lstm_ins = [lasagne.layers.get_output(l_lstm_in[iLayer]),\n",
    "                lasagne.layers.get_output(l_h_init[iLayer]), \n",
    "                lasagne.layers.get_output(l_c_init[iLayer])]\n",
    "    hid_out.append(l_lstm[iLayer].get_output_for(lstm_ins,cellsc))\n",
    "    cell_out.append(cellsc[l_lstm[iLayer]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_out_resh = T.reshape(lstm_out, (lstm_out.shape[0] * lstm_out.shape[1], -1))\n",
    "y_logit = T.dot(lstm_out_resh,W_hy) + b_y\n",
    "y_hat = T.nnet.softmax(y_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_var = T.imatrix('target variable').astype('int32')\n",
    "target_var_f = target_var.flatten(ndim=1)\n",
    "seq_cost = -T.log(y_hat)[T.arange(y_hat.shape[0]),target_var_f]\n",
    "resh_cost = T.reshape(seq_cost, (lstm_out.shape[0], lstm_out.shape[1]))\n",
    "cost = T.sum(T.mean(resh_cost,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f_seq_cost = theano.function([input_var,target_var,drate],cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_xx = np.repeat([test_x[0]],batch_size,0)\n",
    "# test_yy = np.repeat([test_y[0]],batch_size,0)\n",
    "# np.exp(f_seq_cost(test_xx,test_yy,0.)/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_all = [embed]+p_lstm+[W_hy,b_y]\n",
    "grads_all = T.grad(cost,p_all)\n",
    "\n",
    "norm_grads_all = 0\n",
    "for grads in grads_all:\n",
    "    norm_grads_all += T.sum(grads ** 2)\n",
    "norm_grads_all = T.sqrt(norm_grads_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# p_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gshared = [theano.shared(p.get_value() * 0.) for p in p_all]\n",
    "shrink_factor = ifelse(T.gt(norm_grads_all,grad_clip),grad_clip/norm_grads_all,1.)\n",
    "gup = [(gs,g*shrink_factor) for gs,g in zip(gshared,grads_all)] # gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry over hidden and cell states for the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gup = theano.compat.OrderedDict(gup)\n",
    "for iLayer in range(n_layer):\n",
    "    gup[c_init[iLayer]] = cell_out[iLayer][-1]\n",
    "    gup[h_init[iLayer]] = hid_out[iLayer].dimshuffle(1, 0, 2)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_grad_shared = theano.function([input_var, target_var],cost,updates=gup, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the parameters with specified learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = T.iscalar().astype(T.config.floatX)\n",
    "pup = [(p,p - lrate * g) for p,g in zip(p_all,gshared)]\n",
    "f_update = theano.function([lrate],[],updates=pup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative optimization procedure: adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if optim_type=='adadelta':\n",
    "    scaled_grads = lasagne.updates.total_norm_constraint(grads_all, grad_clip)\n",
    "    adaup = lasagne.updates.adadelta(scaled_grads, p_all, learning_rate, rho=.9)\n",
    "    adaup = theano.compat.OrderedDict(adaup)\n",
    "    for iLayer in range(n_layer):\n",
    "        adaup[c_init[iLayer]] = cell_out[iLayer][-1]\n",
    "        adaup[h_init[iLayer]] = hid_out[iLayer].dimshuffle(1, 0, 2)[-1]\n",
    "    f_ada_update = theano.function([input_var, target_var],cost,updates=adaup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reset the hidden and cell states to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rup = theano.compat.OrderedDict()\n",
    "for iLayer in range(n_layer):\n",
    "    rup[c_init[iLayer]] = theano.shared(zero_fcn((batch_size,embed_size)))\n",
    "    rup[h_init[iLayer]] = theano.shared(zero_fcn((batch_size,embed_size)))\n",
    "f_reset_state = theano.function([],[],updates=rup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f_grad_norm = theano.function([input_var, target_var], norm_grads_all)\n",
    "f_cost = theano.function([input_var, target_var, drate], cost, updates=gup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 8.57 percent: training perplexity 930.969\n",
      "time: 34.69767618179321\n",
      "epoch 0, 17.18 percent: training perplexity 683.096\n",
      "time: 34.643112897872925\n",
      "epoch 0, 25.79 percent: training perplexity 555.802\n",
      "time: 34.69820022583008\n",
      "epoch 0, 34.40 percent: training perplexity 477.477\n",
      "time: 34.667715549468994\n",
      "epoch 0, 43.00 percent: training perplexity 424.574\n",
      "time: 34.658427000045776\n",
      "epoch 0, 51.61 percent: training perplexity 387.441\n",
      "time: 34.71850514411926\n",
      "epoch 0, 60.22 percent: training perplexity 354.260\n",
      "time: 34.639967918395996\n",
      "epoch 0, 68.83 percent: training perplexity 330.121\n",
      "time: 34.670252323150635\n",
      "epoch 0, 77.44 percent: training perplexity 312.332\n",
      "time: 34.914453744888306\n",
      "epoch 0, 86.05 percent: training perplexity 294.283\n",
      "time: 34.917240858078\n",
      "epoch 0, 94.66 percent: training perplexity 277.532\n",
      "time: 34.907180309295654\n",
      "\n",
      "======================================\n",
      "valid perplexity: 179.057\n",
      "======================================\n",
      "epoch 1, 8.57 percent: training perplexity 152.177\n",
      "time: 88.91546082496643\n",
      "epoch 1, 17.18 percent: training perplexity 158.776\n",
      "time: 35.17806148529053\n",
      "epoch 1, 25.79 percent: training perplexity 155.646\n",
      "time: 34.877686738967896\n",
      "epoch 1, 34.40 percent: training perplexity 152.245\n",
      "time: 34.841092586517334\n",
      "epoch 1, 43.00 percent: training perplexity 150.195\n",
      "time: 34.76026153564453\n",
      "epoch 1, 51.61 percent: training perplexity 148.279\n",
      "time: 34.77174663543701\n",
      "epoch 1, 60.22 percent: training perplexity 144.450\n",
      "time: 34.7363965511322\n",
      "epoch 1, 68.83 percent: training perplexity 142.298\n",
      "time: 34.66976594924927\n",
      "epoch 1, 77.44 percent: training perplexity 141.546\n",
      "time: 34.78610014915466\n",
      "epoch 1, 86.05 percent: training perplexity 138.313\n",
      "time: 34.724244356155396\n",
      "epoch 1, 94.66 percent: training perplexity 134.949\n",
      "time: 35.3759241104126\n",
      "\n",
      "======================================\n",
      "valid perplexity: 143.632\n",
      "======================================\n",
      "epoch 2, 8.57 percent: training perplexity 105.839\n",
      "time: 88.22506666183472\n",
      "epoch 2, 17.18 percent: training perplexity 113.904\n",
      "time: 34.7376983165741\n",
      "epoch 2, 25.79 percent: training perplexity 112.807\n",
      "time: 34.726524353027344\n",
      "epoch 2, 34.40 percent: training perplexity 111.190\n",
      "time: 34.75877594947815\n",
      "epoch 2, 43.00 percent: training perplexity 110.814\n",
      "time: 34.77707123756409\n",
      "epoch 2, 51.61 percent: training perplexity 110.155\n",
      "time: 35.103607416152954\n",
      "epoch 2, 60.22 percent: training perplexity 107.961\n",
      "time: 35.11403441429138\n",
      "epoch 2, 68.83 percent: training perplexity 107.155\n",
      "time: 35.09186267852783\n",
      "epoch 2, 77.44 percent: training perplexity 107.399\n",
      "time: 34.635884046554565\n",
      "epoch 2, 86.05 percent: training perplexity 105.437\n",
      "time: 34.61499524116516\n",
      "epoch 2, 94.66 percent: training perplexity 103.402\n",
      "time: 34.71468901634216\n",
      "\n",
      "======================================\n",
      "valid perplexity: 132.778\n",
      "======================================\n",
      "epoch 3, 8.57 percent: training perplexity 85.754\n",
      "time: 87.97350192070007\n",
      "epoch 3, 17.18 percent: training perplexity 93.385\n",
      "time: 35.359848499298096\n",
      "epoch 3, 25.79 percent: training perplexity 92.716\n",
      "time: 35.570885181427\n",
      "epoch 3, 34.40 percent: training perplexity 91.695\n",
      "time: 34.779117822647095\n",
      "epoch 3, 43.00 percent: training perplexity 91.662\n",
      "time: 34.64007782936096\n",
      "epoch 3, 51.61 percent: training perplexity 91.371\n",
      "time: 34.757251024246216\n",
      "epoch 3, 60.22 percent: training perplexity 89.791\n",
      "time: 35.02434206008911\n",
      "epoch 3, 68.83 percent: training perplexity 89.360\n",
      "time: 35.18645477294922\n",
      "epoch 3, 77.44 percent: training perplexity 89.826\n",
      "time: 34.9341025352478\n",
      "epoch 3, 86.05 percent: training perplexity 88.324\n",
      "time: 35.4730863571167\n",
      "epoch 3, 94.66 percent: training perplexity 86.849\n",
      "time: 35.18130898475647\n",
      "\n",
      "======================================\n",
      "valid perplexity: 128.991\n",
      "======================================\n",
      "epoch 4, 8.57 percent: training perplexity 74.033\n",
      "time: 88.97167682647705\n",
      "epoch 4, 17.18 percent: training perplexity 80.811\n",
      "time: 34.719319105148315\n",
      "epoch 4, 25.79 percent: training perplexity 80.348\n",
      "time: 34.770103454589844\n",
      "epoch 4, 34.40 percent: training perplexity 79.659\n",
      "time: 34.72613024711609\n",
      "epoch 4, 43.00 percent: training perplexity 79.844\n",
      "time: 34.68512415885925\n",
      "epoch 4, 51.61 percent: training perplexity 79.769\n",
      "time: 34.64600205421448\n",
      "epoch 4, 60.22 percent: training perplexity 78.581\n",
      "time: 34.62397241592407\n",
      "epoch 4, 68.83 percent: training perplexity 78.422\n",
      "time: 34.80306911468506\n",
      "epoch 4, 77.44 percent: training perplexity 78.983\n",
      "time: 34.75103759765625\n",
      "epoch 4, 86.05 percent: training perplexity 77.806\n",
      "time: 34.83913969993591\n",
      "epoch 4, 94.66 percent: training perplexity 76.599\n",
      "time: 35.064138650894165\n",
      "\n",
      "======================================\n",
      "valid perplexity: 127.396\n",
      "======================================\n",
      "epoch 5, 8.57 percent: training perplexity 66.552\n",
      "time: 89.19801640510559\n",
      "epoch 5, 17.18 percent: training perplexity 72.709\n",
      "time: 35.51990485191345\n",
      "epoch 5, 25.79 percent: training perplexity 72.399\n",
      "time: 34.90283942222595\n",
      "epoch 5, 34.40 percent: training perplexity 71.887\n",
      "time: 35.04171013832092\n",
      "epoch 5, 43.00 percent: training perplexity 72.114\n",
      "time: 35.389490842819214\n",
      "epoch 5, 51.61 percent: training perplexity 72.158\n",
      "time: 35.28357291221619\n",
      "epoch 5, 60.22 percent: training perplexity 71.200\n",
      "time: 34.82543349266052\n",
      "epoch 5, 68.83 percent: training perplexity 71.081\n",
      "time: 34.71845865249634\n",
      "epoch 5, 77.44 percent: training perplexity 71.685\n",
      "time: 35.28328561782837\n",
      "epoch 5, 86.05 percent: training perplexity 70.747\n",
      "time: 35.56250858306885\n",
      "epoch 5, 94.66 percent: training perplexity 69.738\n",
      "time: 35.194878816604614\n",
      "\n",
      "======================================\n",
      "valid perplexity: 127.697\n",
      "======================================\n",
      "epoch 6, 8.57 percent: training perplexity 59.759\n",
      "time: 88.59983658790588\n",
      "epoch 6, 17.18 percent: training perplexity 64.215\n",
      "time: 35.9571053981781\n",
      "epoch 6, 25.79 percent: training perplexity 62.868\n",
      "time: 35.07131838798523\n",
      "epoch 6, 34.40 percent: training perplexity 61.627\n",
      "time: 34.800925493240356\n",
      "epoch 6, 43.00 percent: training perplexity 61.108\n",
      "time: 34.721386194229126\n",
      "epoch 6, 51.61 percent: training perplexity 60.489\n",
      "time: 34.83471393585205\n",
      "epoch 6, 60.22 percent: training perplexity 59.016\n",
      "time: 35.55579662322998\n",
      "epoch 6, 68.83 percent: training perplexity 58.375\n",
      "time: 35.004241943359375\n",
      "epoch 6, 77.44 percent: training perplexity 58.244\n",
      "time: 34.883228063583374\n",
      "epoch 6, 86.05 percent: training perplexity 56.879\n",
      "time: 34.88382434844971\n",
      "epoch 6, 94.66 percent: training perplexity 55.503\n",
      "time: 34.86547040939331\n",
      "\n",
      "======================================\n",
      "valid perplexity: 122.763\n",
      "======================================\n",
      "epoch 7, 8.57 percent: training perplexity 49.530\n",
      "time: 87.92440795898438\n",
      "epoch 7, 17.18 percent: training perplexity 53.351\n",
      "time: 34.6570782661438\n",
      "epoch 7, 25.79 percent: training perplexity 52.150\n",
      "time: 34.696009397506714\n",
      "epoch 7, 34.40 percent: training perplexity 51.071\n",
      "time: 34.79661321640015\n",
      "epoch 7, 43.00 percent: training perplexity 50.567\n",
      "time: 35.0846221446991\n",
      "epoch 7, 51.61 percent: training perplexity 50.013\n",
      "time: 35.19757604598999\n",
      "epoch 7, 60.22 percent: training perplexity 48.683\n",
      "time: 35.26802587509155\n",
      "epoch 7, 68.83 percent: training perplexity 48.058\n",
      "time: 35.13622689247131\n",
      "epoch 7, 77.44 percent: training perplexity 47.822\n",
      "time: 35.09272885322571\n",
      "epoch 7, 86.05 percent: training perplexity 46.564\n",
      "time: 35.09860396385193\n",
      "epoch 7, 94.66 percent: training perplexity 45.306\n",
      "time: 35.05741810798645\n",
      "\n",
      "======================================\n",
      "valid perplexity: 123.038\n",
      "======================================\n",
      "epoch 8, 8.57 percent: training perplexity 43.740\n",
      "time: 88.65057873725891\n",
      "epoch 8, 17.18 percent: training perplexity 47.283\n",
      "time: 35.121910095214844\n",
      "epoch 8, 25.79 percent: training perplexity 46.240\n",
      "time: 35.20503258705139\n",
      "epoch 8, 34.40 percent: training perplexity 45.280\n",
      "time: 35.13253593444824\n",
      "epoch 8, 43.00 percent: training perplexity 44.818\n",
      "time: 35.142738580703735\n",
      "epoch 8, 51.61 percent: training perplexity 44.291\n",
      "time: 35.12100005149841\n",
      "epoch 8, 60.22 percent: training perplexity 43.074\n",
      "time: 35.03674507141113\n",
      "epoch 8, 68.83 percent: training perplexity 42.488\n",
      "time: 34.9172580242157\n",
      "epoch 8, 77.44 percent: training perplexity 42.220\n",
      "time: 35.04694843292236\n",
      "epoch 8, 86.05 percent: training perplexity 41.051\n",
      "time: 34.83165264129639\n",
      "epoch 8, 94.66 percent: training perplexity 39.895\n",
      "time: 35.56419825553894\n",
      "\n",
      "======================================\n",
      "valid perplexity: 125.140\n",
      "======================================\n",
      "epoch 9, 8.57 percent: training perplexity 40.894\n",
      "time: 88.61285996437073\n",
      "epoch 9, 17.18 percent: training perplexity 44.243\n",
      "time: 34.62832593917847\n",
      "epoch 9, 25.79 percent: training perplexity 43.270\n",
      "time: 35.05974507331848\n",
      "epoch 9, 34.40 percent: training perplexity 42.369\n",
      "time: 34.96968698501587\n",
      "epoch 9, 43.00 percent: training perplexity 41.944\n",
      "time: 34.69253706932068\n",
      "epoch 9, 51.61 percent: training perplexity 41.435\n",
      "time: 34.95935678482056\n",
      "epoch 9, 60.22 percent: training perplexity 40.271\n",
      "time: 35.635231018066406\n",
      "epoch 9, 68.83 percent: training perplexity 39.706\n",
      "time: 35.50036954879761\n",
      "epoch 9, 77.44 percent: training perplexity 39.430\n",
      "time: 35.840094327926636\n",
      "epoch 9, 86.05 percent: training perplexity 38.307\n",
      "time: 35.062477827072144\n",
      "epoch 9, 94.66 percent: training perplexity 37.197\n",
      "time: 34.83567547798157\n",
      "\n",
      "======================================\n",
      "valid perplexity: 126.320\n",
      "======================================\n",
      "epoch 10, 8.57 percent: training perplexity 39.506\n",
      "time: 89.05499768257141\n",
      "epoch 10, 17.18 percent: training perplexity 42.741\n",
      "time: 36.019508838653564\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ca9d3e0aa633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptim_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf_grad_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mf_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meg/meg1/users/peterd/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/meg/meg1/users/peterd/anaconda3/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_look = 200\n",
    "\n",
    "import time\n",
    "loss_val = 0; i_batch = 0; i_epoch = 0\n",
    "a = time.time()\n",
    "gnorm = []\n",
    "\n",
    "for i_epoch in range(num_epoch):\n",
    "\n",
    "    for (x,y,p) in data_producer(data,batch_size):\n",
    "\n",
    "        if optim_type=='sgd':\n",
    "            loss_val += f_grad_shared(x,y)/batch_size\n",
    "            f_update(learning_rate)\n",
    "        else:\n",
    "            loss_val += f_ada_update(x,y)\n",
    "        i_batch += 1\n",
    "\n",
    "        if ((i_batch%n_look)==0):\n",
    "            print('epoch %d, %.2f percent: training perplexity %.3f' \n",
    "                  %(i_epoch,(p*100),np.exp(loss_val/i_batch)))\n",
    "            print('time: '+str(time.time()-a))\n",
    "            a = time.time()\n",
    "            \n",
    "    f_reset_state()\n",
    "    valid_cost = 0; i_batch = 0\n",
    "    for (x,y,p) in data_producer(data_valid,batch_size):\n",
    "        valid_cost += f_cost(x,y,0.)/batch_size # dropout-rate: 0.\n",
    "        i_batch += 1\n",
    "    tp = np.exp(valid_cost/i_batch)\n",
    "    print('')\n",
    "    print('======================================')\n",
    "    print('valid perplexity: %.3f' %(tp))\n",
    "    print('======================================')\n",
    "    f_reset_state()\n",
    "    i_batch = 0; loss_val = 0\n",
    "    \n",
    "#     if is_sgd:\n",
    "    if i_epoch > max_epoch:\n",
    "        learning_rate /= decay # adjust learning rate        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "test perplexity: 118.384\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "f_reset_state()\n",
    "test_cost = 0; i_batch = 0\n",
    "for (x,y,p) in data_producer(data_test,batch_size):\n",
    "    test_cost += f_cost(x,y,0.)/batch_size\n",
    "    i_batch += 1\n",
    "tp = np.exp(test_cost/i_batch)\n",
    "print('')\n",
    "print('======================================')\n",
    "print('test perplexity: %.3f' %(tp))\n",
    "print('======================================')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
